vocab_size: 32000
n_position: 95
model_dim: 768
ff_dim: 2048
head: 8
n_layers: 6
dropout_rate: 0.1
n_epochs: 10
batch_size: 32
seed: 116
learning_rate: 1.0e-5
betas: [0.9, 0.98]
max_grad_norm: 1.0
warmup: 128000
max_len: 95
bert_model_name: bert-base-uncased
train_data: data/train_data.pkl
text_data: data/processed_cornell_data.txt
data_dir: data
fn: trained_model
file_path: data/processed_cornell_data.txt

# This is for training from a previous saved model in Kaggle if load is True
load: False
ckpt_path: ../../input/trained-model/trained_model.pth